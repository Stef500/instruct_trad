# Medical Dataset Processor

Un syst√®me automatis√© pour le traitement de datasets m√©dicaux combinant traduction automatique via DeepL et g√©n√©ration de contenu via OpenAI GPT-4o-mini.

## üìö Documentation

La documentation compl√®te se trouve dans le r√©pertoire `docs/` :

- **[Interface Web de Traduction](docs/WEB_INTERFACE_USAGE.md)** - Guide complet pour l'interface web interactive
- **[D√©ploiement Docker](docs/DOCKER_DEPLOYMENT.md)** - Instructions de d√©ploiement avec Docker
- **[Exemples de Configuration](docs/examples/)** - Fichiers d'exemple et cas d'usage

## Vue d'ensemble

Ce package automatise le traitement de datasets m√©dicaux en:
- R√©cup√©rant des datasets m√©dicaux depuis Hugging Face
- Traduisant 50 √©chantillons par dataset via l'API DeepL
- G√©n√©rant du contenu pour 50 autres √©chantillons via OpenAI GPT-4o-mini
- Consolidant les r√©sultats au format JSONL
- Cr√©ant un √©chantillon PDF pour relecture

## Datasets support√©s

- **MedQA**: Questions-r√©ponses m√©dicales cliniques
- **PubMedQA**: Questions-r√©ponses bas√©es sur PubMed
- **HealthSearchQA**: Questions-r√©ponses sur la sant√©
- **MMLU Clinical**: Connaissances cliniques du benchmark MMLU

## Installation

### Pr√©requis

- Python 3.8+
- Cl√© API DeepL (https://www.deepl.com/pro-api)
- Cl√© API OpenAI (https://platform.openai.com/api-keys)

### Installation du package

```bash
# Cloner le repository
git clone <repository-url>
cd medical-dataset-processor

# Installer les d√©pendances
pip install -e .

# Ou avec uv (recommand√©)
uv pip install -e .
```

### Configuration

1. Copiez le fichier de configuration d'exemple:
```bash
cp .env.example .env
```

2. √âditez le fichier `.env` avec vos cl√©s API:
```bash
DEEPL_API_KEY=your_deepl_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
```

## Utilisation

### Interface en ligne de commande

```bash
# Traitement complet avec configuration par d√©faut
medical-dataset-processor

# Sp√©cifier des fichiers de configuration personnalis√©s
medical-dataset-processor --config datasets.yaml --output-dir ./output

# Traitement avec options avanc√©es
medical-dataset-processor \
  --datasets medqa,pubmedqa \
  --translation-samples 25 \
  --generation-samples 25 \
  --target-language fr \
  --pdf-samples 50
```

### Options de la CLI

- `--config`: Fichier de configuration YAML (d√©faut: `datasets.yaml`)
- `--output-dir`: R√©pertoire de sortie (d√©faut: `./output`)
- `--datasets`: Datasets √† traiter (d√©faut: tous)
- `--translation-samples`: Nombre d'√©chantillons √† traduire par dataset (d√©faut: 50)
- `--generation-samples`: Nombre d'√©chantillons √† g√©n√©rer par dataset (d√©faut: 50)
- `--target-language`: Langue cible pour la traduction (d√©faut: fr)
- `--pdf-samples`: Nombre d'√©chantillons dans le PDF (d√©faut: 100)
- `--parallel`: Activer le traitement parall√®le
- `--resume`: Reprendre un traitement interrompu

### Utilisation programmatique

```python
from medical_dataset_processor import MedicalDatasetProcessor

# Initialisation
processor = MedicalDatasetProcessor(
    deepl_api_key="your_deepl_key",
    openai_api_key="your_openai_key"
)

# Traitement complet
results = processor.process_datasets(
    config_path="datasets.yaml",
    output_dir="./output"
)

# Traitement par √©tapes
datasets = processor.load_datasets("datasets.yaml")
translated = processor.translate_samples(datasets, samples_per_dataset=50)
generated = processor.generate_samples(datasets, samples_per_dataset=50)
consolidated = processor.consolidate_datasets(translated, generated)
processor.export_jsonl(consolidated, "output/dataset.jsonl")
processor.generate_pdf_sample(consolidated, "output/sample.pdf")
```

## Configuration

### Fichier datasets.yaml

Le fichier `datasets.yaml` d√©finit les datasets √† traiter:

```yaml
medqa:
  name: "medqa"
  source_type: "huggingface"
  source_path: "bigbio/med_qa"
  subset: "med_qa_en_bigbio_qa"
  text_fields: ["question", "answer"]
  description: "Medical Question Answering dataset"

processing_config:
  translation_samples_per_dataset: 50
  generation_samples_per_dataset: 50
  target_language: "fr"
  output_format: "jsonl"
```

### Variables d'environnement

Toutes les options peuvent √™tre configur√©es via des variables d'environnement:

```bash
# APIs
DEEPL_API_KEY=your_key
OPENAI_API_KEY=your_key
TARGET_LANGUAGE=fr
OPENAI_MODEL=gpt-4o-mini

# Sortie
OUTPUT_JSONL_PATH=output/dataset.jsonl
OUTPUT_PDF_PATH=output/sample.pdf

# Performance
MAX_CONCURRENT_REQUESTS=5
MAX_RETRIES=3
REQUEST_TIMEOUT=30
```

## Format de sortie

### Fichier JSONL

Chaque ligne du fichier JSONL contient un √©chantillon trait√©:

```json
{
  "id": "medqa_001",
  "source_dataset": "medqa",
  "processing_type": "translation",
  "original_text": "What is the most common cause of...",
  "processed_text": "Quelle est la cause la plus fr√©quente de...",
  "metadata": {
    "timestamp": "2024-01-15T10:30:00Z",
    "api_used": "deepl",
    "confidence_score": 0.95
  }
}
```

### √âchantillon PDF

Le PDF contient 100 √©chantillons al√©atoires format√©s pour relecture humaine avec:
- Texte original
- Texte traduit/g√©n√©r√©
- M√©tadonn√©es de traitement
- Scores de qualit√©

## Gestion d'erreurs et reprise

Le syst√®me inclut une gestion robuste des erreurs:

- **Retry automatique**: Jusqu'√† 3 tentatives par √©chantillon
- **Rate limiting**: Gestion automatique des limites d'API
- **Sauvegarde d'√©tat**: Reprise possible apr√®s interruption
- **Logging d√©taill√©**: Tra√ßabilit√© compl√®te des op√©rations

### Reprendre un traitement interrompu

```bash
# Le syst√®me d√©tecte automatiquement les traitements interrompus
medical-dataset-processor --resume

# Ou sp√©cifier un fichier d'√©tat
medical-dataset-processor --resume --state-file ./logs/processing_state.json
```

## Logs et monitoring

Les logs sont sauvegard√©s dans `./logs/` avec:
- Progression du traitement
- Erreurs d'API d√©taill√©es
- Statistiques de performance
- Rapports de qualit√©

```bash
# Voir les logs en temps r√©el
tail -f logs/medical_dataset_processor.log

# Analyser les erreurs
grep "ERROR" logs/medical_dataset_processor.log
```

## D√©veloppement

### Structure du projet

```
src/medical_dataset_processor/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ cli.py                 # Interface ligne de commande
‚îú‚îÄ‚îÄ pipeline.py           # Orchestrateur principal
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ core.py          # Mod√®les de donn√©es
‚îú‚îÄ‚îÄ loaders/
‚îÇ   ‚îî‚îÄ‚îÄ dataset_loader.py # Chargement des datasets
‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îú‚îÄ‚îÄ translation_processor.py
‚îÇ   ‚îú‚îÄ‚îÄ generation_processor.py
‚îÇ   ‚îú‚îÄ‚îÄ sample_selector.py
‚îÇ   ‚îî‚îÄ‚îÄ dataset_consolidator.py
‚îú‚îÄ‚îÄ exporters/
‚îÇ   ‚îú‚îÄ‚îÄ jsonl_exporter.py
‚îÇ   ‚îî‚îÄ‚îÄ pdf_sample_generator.py
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ logging.py
    ‚îî‚îÄ‚îÄ state_recovery.py
```

### Tests

```bash
# Lancer tous les tests
pytest

# Tests avec couverture
pytest --cov=src/medical_dataset_processor

# Tests d'int√©gration uniquement
pytest tests/test_*_integration.py
```

### Contribution

1. Fork le repository
2. Cr√©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit les changements (`git commit -am 'Ajouter nouvelle fonctionnalit√©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Cr√©er une Pull Request

## Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## Support

Pour des questions ou probl√®mes:
1. Consulter la documentation
2. V√©rifier les issues existantes
3. Cr√©er une nouvelle issue avec les d√©tails du probl√®me